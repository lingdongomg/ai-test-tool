"""
åŠŸèƒ½3: æ—¥å¿—å¼‚å¸¸æ£€æµ‹ä¸å‘Šè­¦æœåŠ¡
è§£ææ—¥å¿—ä¸­çš„ warning/error ä¿¡æ¯ï¼Œæ£€æµ‹å¼‚å¸¸ï¼Œç”Ÿæˆå‘Šè­¦æŠ¥å‘Š
"""

import json
import re
import hashlib
from typing import Any
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from collections import defaultdict

from ..database import get_db_manager
from ..llm.chains import LogAnalysisChain, ReportGeneratorChain
from ..llm.provider import get_llm_provider
from ..utils.logger import get_logger


class AnomalySeverity(Enum):
    """å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
    CRITICAL = "critical"    # ä¸¥é‡
    ERROR = "error"          # é”™è¯¯
    WARNING = "warning"      # è­¦å‘Š
    INFO = "info"            # ä¿¡æ¯


class AnomalyType(Enum):
    """å¼‚å¸¸ç±»å‹"""
    ERROR_LOG = "error_log"              # é”™è¯¯æ—¥å¿—
    WARNING_LOG = "warning_log"          # è­¦å‘Šæ—¥å¿—
    EXCEPTION = "exception"              # å¼‚å¸¸å †æ ˆ
    TIMEOUT = "timeout"                  # è¶…æ—¶
    HIGH_LATENCY = "high_latency"        # é«˜å»¶è¿Ÿ
    ERROR_RATE_SPIKE = "error_rate_spike"  # é”™è¯¯ç‡é£™å‡
    TRAFFIC_ANOMALY = "traffic_anomaly"  # æµé‡å¼‚å¸¸
    SECURITY_ALERT = "security_alert"    # å®‰å…¨å‘Šè­¦


@dataclass
class LogAnomaly:
    """æ—¥å¿—å¼‚å¸¸"""
    anomaly_id: str
    anomaly_type: AnomalyType
    severity: AnomalySeverity
    title: str
    description: str
    log_content: str
    timestamp: datetime | None = None
    count: int = 1
    affected_endpoints: list[str] = field(default_factory=list)
    stack_trace: str | None = None
    suggested_actions: list[str] = field(default_factory=list)
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class AnomalyReport:
    """å¼‚å¸¸æŠ¥å‘Š"""
    report_id: str
    task_id: str
    title: str
    summary: str
    total_anomalies: int
    critical_count: int
    error_count: int
    warning_count: int
    anomalies: list[LogAnomaly]
    ai_analysis: str | None = None
    recommendations: list[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)


class LogAnomalyDetectorService:
    """
    æ—¥å¿—å¼‚å¸¸æ£€æµ‹æœåŠ¡
    
    åŠŸèƒ½ï¼š
    1. è§£ææ—¥å¿—ä¸­çš„ warning/error ä¿¡æ¯
    2. æ£€æµ‹å¼‚å¸¸æ¨¡å¼ï¼ˆè¶…æ—¶ã€é«˜å»¶è¿Ÿã€é”™è¯¯ç‡ç­‰ï¼‰
    3. ä½¿ç”¨ AI åˆ†æå¼‚å¸¸åŸå› 
    4. ç”Ÿæˆå¼‚å¸¸æŠ¥å‘Š
    """
    
    # å¸¸è§é”™è¯¯æ¨¡å¼
    ERROR_PATTERNS = [
        (r'\b(ERROR|FATAL|CRITICAL)\b', AnomalySeverity.ERROR),
        (r'\b(Exception|Error|Failure)\b.*?:', AnomalySeverity.ERROR),
        (r'(?i)(failed|failure|error|exception)', AnomalySeverity.ERROR),
        (r'(?i)(timeout|timed out)', AnomalySeverity.ERROR),
        (r'(?i)(connection refused|connection reset)', AnomalySeverity.ERROR),
        (r'(?i)(out of memory|oom)', AnomalySeverity.CRITICAL),
        (r'(?i)(deadlock|race condition)', AnomalySeverity.CRITICAL),
    ]
    
    WARNING_PATTERNS = [
        (r'\b(WARN|WARNING)\b', AnomalySeverity.WARNING),
        (r'(?i)(deprecated|deprecation)', AnomalySeverity.WARNING),
        (r'(?i)(slow query|slow request)', AnomalySeverity.WARNING),
        (r'(?i)(retry|retrying)', AnomalySeverity.WARNING),
        (r'(?i)(high memory|high cpu)', AnomalySeverity.WARNING),
    ]
    
    SECURITY_PATTERNS = [
        (r'(?i)(sql injection|xss|csrf)', AnomalySeverity.CRITICAL),
        (r'(?i)(unauthorized|forbidden|access denied)', AnomalySeverity.ERROR),
        (r'(?i)(invalid token|token expired)', AnomalySeverity.WARNING),
        (r'(?i)(brute force|too many attempts)', AnomalySeverity.ERROR),
    ]
    
    # å¼‚å¸¸å †æ ˆæ¨¡å¼
    STACK_TRACE_PATTERNS = [
        r'Traceback \(most recent call last\):[\s\S]*?(?=\n\n|\Z)',  # Python
        r'at [\w.$]+\([\w.]+:\d+\)[\s\S]*?(?=\n\n|\Z)',  # Java
        r'Error:.*\n\s+at .*\n(?:\s+at .*\n)*',  # JavaScript
    ]
    
    def __init__(self, verbose: bool = False):
        self.logger = get_logger(verbose)
        self.verbose = verbose
        self.db = get_db_manager()
        self._analysis_chain: LogAnalysisChain | None = None
        self._report_chain: ReportGeneratorChain | None = None
    
    @property
    def analysis_chain(self) -> LogAnalysisChain:
        """æ‡’åŠ è½½åˆ†æ Chain"""
        if self._analysis_chain is None:
            provider = get_llm_provider()
            self._analysis_chain = LogAnalysisChain(provider, self.verbose)
        return self._analysis_chain
    
    @property
    def report_chain(self) -> ReportGeneratorChain:
        """æ‡’åŠ è½½æŠ¥å‘Š Chain"""
        if self._report_chain is None:
            provider = get_llm_provider()
            self._report_chain = ReportGeneratorChain(provider, self.verbose)
        return self._report_chain
    
    def detect_anomalies_from_task(
        self,
        task_id: str,
        include_ai_analysis: bool = True
    ) -> AnomalyReport:
        """
        ä»åˆ†æä»»åŠ¡ä¸­æ£€æµ‹å¼‚å¸¸
        
        Args:
            task_id: åˆ†æä»»åŠ¡ID
            include_ai_analysis: æ˜¯å¦åŒ…å«AIåˆ†æ
            
        Returns:
            å¼‚å¸¸æŠ¥å‘Š
        """
        self.logger.start_step("æ£€æµ‹æ—¥å¿—å¼‚å¸¸")
        
        # è·å–ä»»åŠ¡ä¸­çš„è¯·æ±‚è®°å½•
        sql = """
            SELECT * FROM parsed_requests 
            WHERE task_id = %s
            ORDER BY timestamp
        """
        requests = self.db.fetch_all(sql, (task_id,))
        
        anomalies: list[LogAnomaly] = []
        
        # 1. æ£€æµ‹é”™è¯¯å’Œè­¦å‘Šæ—¥å¿—
        error_anomalies = self._detect_error_logs(requests)
        anomalies.extend(error_anomalies)
        
        # 2. æ£€æµ‹å¼‚å¸¸å †æ ˆ
        exception_anomalies = self._detect_exceptions(requests)
        anomalies.extend(exception_anomalies)
        
        # 3. æ£€æµ‹æ€§èƒ½å¼‚å¸¸
        perf_anomalies = self._detect_performance_anomalies(requests)
        anomalies.extend(perf_anomalies)
        
        # 4. æ£€æµ‹å®‰å…¨å¼‚å¸¸
        security_anomalies = self._detect_security_anomalies(requests)
        anomalies.extend(security_anomalies)
        
        # 5. æ£€æµ‹é”™è¯¯ç‡å¼‚å¸¸
        rate_anomalies = self._detect_error_rate_anomalies(requests)
        anomalies.extend(rate_anomalies)
        
        # å»é‡å’Œèšåˆ
        anomalies = self._aggregate_anomalies(anomalies)
        
        # ç»Ÿè®¡
        critical_count = sum(1 for a in anomalies if a.severity == AnomalySeverity.CRITICAL)
        error_count = sum(1 for a in anomalies if a.severity == AnomalySeverity.ERROR)
        warning_count = sum(1 for a in anomalies if a.severity == AnomalySeverity.WARNING)
        
        self.logger.info(f"æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸: {critical_count} ä¸¥é‡, {error_count} é”™è¯¯, {warning_count} è­¦å‘Š")
        
        # AI åˆ†æ
        ai_analysis = None
        recommendations: list[str] = []
        if include_ai_analysis and anomalies:
            try:
                ai_result = self._ai_analyze_anomalies(anomalies)
                ai_analysis = ai_result.get('analysis', '')
                recommendations = ai_result.get('recommendations', [])
            except Exception as e:
                self.logger.warn(f"AI åˆ†æå¤±è´¥: {e}")
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self._create_report(
            task_id, anomalies, ai_analysis, recommendations
        )
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_report(report)
        
        self.logger.end_step(f"ç”Ÿæˆå¼‚å¸¸æŠ¥å‘Š: {report.report_id}")
        
        return report
    
    def detect_anomalies_from_log_content(
        self,
        log_content: str,
        source_name: str = "manual"
    ) -> list[LogAnomaly]:
        """
        ä»æ—¥å¿—å†…å®¹ç›´æ¥æ£€æµ‹å¼‚å¸¸
        
        Args:
            log_content: æ—¥å¿—å†…å®¹
            source_name: æ¥æºåç§°
            
        Returns:
            å¼‚å¸¸åˆ—è¡¨
        """
        anomalies: list[LogAnomaly] = []
        
        # æŒ‰è¡Œåˆ†æ
        lines = log_content.split('\n')
        
        for i, line in enumerate(lines):
            if not line.strip():
                continue
            
            # æ£€æµ‹é”™è¯¯
            for pattern, severity in self.ERROR_PATTERNS:
                if re.search(pattern, line):
                    anomaly = self._create_anomaly_from_line(
                        line, i + 1, AnomalyType.ERROR_LOG, severity
                    )
                    anomalies.append(anomaly)
                    break
            
            # æ£€æµ‹è­¦å‘Š
            for pattern, severity in self.WARNING_PATTERNS:
                if re.search(pattern, line):
                    anomaly = self._create_anomaly_from_line(
                        line, i + 1, AnomalyType.WARNING_LOG, severity
                    )
                    anomalies.append(anomaly)
                    break
            
            # æ£€æµ‹å®‰å…¨é—®é¢˜
            for pattern, severity in self.SECURITY_PATTERNS:
                if re.search(pattern, line):
                    anomaly = self._create_anomaly_from_line(
                        line, i + 1, AnomalyType.SECURITY_ALERT, severity
                    )
                    anomalies.append(anomaly)
                    break
        
        # æ£€æµ‹å¼‚å¸¸å †æ ˆ
        for pattern in self.STACK_TRACE_PATTERNS:
            matches = re.finditer(pattern, log_content)
            for match in matches:
                stack_trace = match.group(0)
                anomaly = LogAnomaly(
                    anomaly_id=hashlib.md5(stack_trace[:100].encode()).hexdigest()[:16],
                    anomaly_type=AnomalyType.EXCEPTION,
                    severity=AnomalySeverity.ERROR,
                    title="å¼‚å¸¸å †æ ˆ",
                    description="æ£€æµ‹åˆ°å¼‚å¸¸å †æ ˆä¿¡æ¯",
                    log_content=stack_trace[:500],
                    stack_trace=stack_trace
                )
                anomalies.append(anomaly)
        
        return self._aggregate_anomalies(anomalies)
    
    def detect_anomalies_from_file(
        self,
        file_path: str,
        task_id: str,
        detect_types: list[str] | None = None,
        include_ai_analysis: bool = True
    ) -> AnomalyReport:
        """
        ä»æ—¥å¿—æ–‡ä»¶æ£€æµ‹å¼‚å¸¸
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            task_id: å…³è”ä»»åŠ¡ID
            detect_types: è¦æ£€æµ‹çš„å¼‚å¸¸ç±»å‹
            include_ai_analysis: æ˜¯å¦åŒ…å«AIåˆ†æ
            
        Returns:
            å¼‚å¸¸æŠ¥å‘Š
        """
        self.logger.start_step(f"ä»æ–‡ä»¶æ£€æµ‹å¼‚å¸¸: {file_path}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                log_content = f.read()
        except Exception as e:
            self.logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            raise
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies = self.detect_anomalies_from_log_content(log_content, file_path)
        
        # æŒ‰ç±»å‹è¿‡æ»¤
        if detect_types:
            type_set = set(detect_types)
            anomalies = [a for a in anomalies if a.anomaly_type.value in type_set]
        
        # ç»Ÿè®¡
        critical_count = sum(1 for a in anomalies if a.severity == AnomalySeverity.CRITICAL)
        error_count = sum(1 for a in anomalies if a.severity == AnomalySeverity.ERROR)
        warning_count = sum(1 for a in anomalies if a.severity == AnomalySeverity.WARNING)
        
        self.logger.info(f"æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸: {critical_count} ä¸¥é‡, {error_count} é”™è¯¯, {warning_count} è­¦å‘Š")
        
        # AI åˆ†æ
        ai_analysis = None
        recommendations: list[str] = []
        if include_ai_analysis and anomalies:
            try:
                ai_result = self._ai_analyze_anomalies(anomalies)
                ai_analysis = ai_result.get('analysis', '')
                recommendations = ai_result.get('recommendations', [])
            except Exception as e:
                self.logger.warn(f"AI åˆ†æå¤±è´¥: {e}")
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self._create_report(
            task_id, anomalies, ai_analysis, recommendations
        )
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_report(report)
        
        self.logger.end_step(f"ç”Ÿæˆå¼‚å¸¸æŠ¥å‘Š: {report.report_id}")
        
        return report
    
    def _detect_error_logs(self, requests: list[dict[str, Any]]) -> list[LogAnomaly]:
        """æ£€æµ‹é”™è¯¯æ—¥å¿—"""
        anomalies: list[LogAnomaly] = []
        
        for req in requests:
            # æ£€æŸ¥ has_error æ ‡è®°
            if req.get('has_error'):
                error_msg = req.get('error_message', '')
                anomaly = LogAnomaly(
                    anomaly_id=hashlib.md5(f"error:{req['request_id']}".encode()).hexdigest()[:16],
                    anomaly_type=AnomalyType.ERROR_LOG,
                    severity=AnomalySeverity.ERROR,
                    title=f"è¯·æ±‚é”™è¯¯: {req['method']} {req['url'][:50]}",
                    description=error_msg[:500] if error_msg else "è¯·æ±‚å¤„ç†å‡ºé”™",
                    log_content=req.get('raw_logs', '')[:1000] if req.get('raw_logs') else '',
                    affected_endpoints=[f"{req['method']} {req['url']}"]
                )
                anomalies.append(anomaly)
            
            # æ£€æŸ¥ has_warning æ ‡è®°
            if req.get('has_warning'):
                warning_msg = req.get('warning_message', '')
                anomaly = LogAnomaly(
                    anomaly_id=hashlib.md5(f"warning:{req['request_id']}".encode()).hexdigest()[:16],
                    anomaly_type=AnomalyType.WARNING_LOG,
                    severity=AnomalySeverity.WARNING,
                    title=f"è¯·æ±‚è­¦å‘Š: {req['method']} {req['url'][:50]}",
                    description=warning_msg[:500] if warning_msg else "è¯·æ±‚å¤„ç†æœ‰è­¦å‘Š",
                    log_content=req.get('raw_logs', '')[:1000] if req.get('raw_logs') else '',
                    affected_endpoints=[f"{req['method']} {req['url']}"]
                )
                anomalies.append(anomaly)
            
            # æ£€æŸ¥ HTTP é”™è¯¯çŠ¶æ€ç 
            status = req.get('http_status', 0)
            if status >= 500:
                anomaly = LogAnomaly(
                    anomaly_id=hashlib.md5(f"5xx:{req['request_id']}".encode()).hexdigest()[:16],
                    anomaly_type=AnomalyType.ERROR_LOG,
                    severity=AnomalySeverity.ERROR,
                    title=f"æœåŠ¡å™¨é”™è¯¯ {status}: {req['method']} {req['url'][:50]}",
                    description=f"HTTP {status} æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
                    log_content=req.get('response_body', '')[:500] if req.get('response_body') else '',
                    affected_endpoints=[f"{req['method']} {req['url']}"],
                    metadata={"status_code": status}
                )
                anomalies.append(anomaly)
            elif status >= 400:
                anomaly = LogAnomaly(
                    anomaly_id=hashlib.md5(f"4xx:{req['request_id']}".encode()).hexdigest()[:16],
                    anomaly_type=AnomalyType.WARNING_LOG,
                    severity=AnomalySeverity.WARNING,
                    title=f"å®¢æˆ·ç«¯é”™è¯¯ {status}: {req['method']} {req['url'][:50]}",
                    description=f"HTTP {status} å®¢æˆ·ç«¯è¯·æ±‚é”™è¯¯",
                    log_content=req.get('response_body', '')[:500] if req.get('response_body') else '',
                    affected_endpoints=[f"{req['method']} {req['url']}"],
                    metadata={"status_code": status}
                )
                anomalies.append(anomaly)
        
        return anomalies
    
    def _detect_exceptions(self, requests: list[dict[str, Any]]) -> list[LogAnomaly]:
        """æ£€æµ‹å¼‚å¸¸å †æ ˆ"""
        anomalies: list[LogAnomaly] = []
        
        for req in requests:
            raw_logs = req.get('raw_logs', '') or ''
            response_body = req.get('response_body', '') or ''
            
            content = f"{raw_logs}\n{response_body}"
            
            for pattern in self.STACK_TRACE_PATTERNS:
                matches = re.finditer(pattern, content)
                for match in matches:
                    stack_trace = match.group(0)
                    # æå–å¼‚å¸¸ç±»å‹
                    exception_type = "Unknown Exception"
                    type_match = re.search(r'(\w+(?:Error|Exception))', stack_trace)
                    if type_match:
                        exception_type = type_match.group(1)
                    
                    anomaly = LogAnomaly(
                        anomaly_id=hashlib.md5(stack_trace[:100].encode()).hexdigest()[:16],
                        anomaly_type=AnomalyType.EXCEPTION,
                        severity=AnomalySeverity.ERROR,
                        title=f"å¼‚å¸¸: {exception_type}",
                        description=f"åœ¨è¯·æ±‚ {req['method']} {req['url'][:50]} ä¸­æ£€æµ‹åˆ°å¼‚å¸¸",
                        log_content=stack_trace[:500],
                        stack_trace=stack_trace,
                        affected_endpoints=[f"{req['method']} {req['url']}"]
                    )
                    anomalies.append(anomaly)
        
        return anomalies
    
    def _detect_performance_anomalies(self, requests: list[dict[str, Any]]) -> list[LogAnomaly]:
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        anomalies: list[LogAnomaly] = []
        
        # è®¡ç®—å“åº”æ—¶é—´ç»Ÿè®¡
        response_times = [
            float(req.get('response_time_ms', 0))
            for req in requests
            if req.get('response_time_ms')
        ]
        
        if not response_times:
            return anomalies
        
        avg_time = sum(response_times) / len(response_times)
        
        # æ£€æµ‹é«˜å»¶è¿Ÿè¯·æ±‚ï¼ˆè¶…è¿‡å¹³å‡å€¼3å€æˆ–è¶…è¿‡5ç§’ï¼‰
        threshold = max(avg_time * 3, 5000)
        
        for req in requests:
            response_time = float(req.get('response_time_ms', 0))
            if response_time > threshold:
                anomaly = LogAnomaly(
                    anomaly_id=hashlib.md5(f"slow:{req['request_id']}".encode()).hexdigest()[:16],
                    anomaly_type=AnomalyType.HIGH_LATENCY,
                    severity=AnomalySeverity.WARNING if response_time < 10000 else AnomalySeverity.ERROR,
                    title=f"é«˜å»¶è¿Ÿè¯·æ±‚: {response_time:.0f}ms",
                    description=f"è¯·æ±‚ {req['method']} {req['url'][:50]} å“åº”æ—¶é—´ {response_time:.0f}msï¼Œè¶…è¿‡é˜ˆå€¼ {threshold:.0f}ms",
                    log_content=f"å“åº”æ—¶é—´: {response_time}ms, å¹³å‡: {avg_time:.0f}ms",
                    affected_endpoints=[f"{req['method']} {req['url']}"],
                    metadata={
                        "response_time_ms": response_time,
                        "threshold_ms": threshold,
                        "avg_time_ms": avg_time
                    }
                )
                anomalies.append(anomaly)
            
            # æ£€æµ‹è¶…æ—¶
            if response_time > 30000:  # 30ç§’
                anomaly = LogAnomaly(
                    anomaly_id=hashlib.md5(f"timeout:{req['request_id']}".encode()).hexdigest()[:16],
                    anomaly_type=AnomalyType.TIMEOUT,
                    severity=AnomalySeverity.ERROR,
                    title=f"è¯·æ±‚è¶…æ—¶: {response_time:.0f}ms",
                    description=f"è¯·æ±‚ {req['method']} {req['url'][:50]} å¯èƒ½è¶…æ—¶",
                    log_content="",
                    affected_endpoints=[f"{req['method']} {req['url']}"],
                    metadata={"response_time_ms": response_time}
                )
                anomalies.append(anomaly)
        
        return anomalies
    
    def _detect_security_anomalies(self, requests: list[dict[str, Any]]) -> list[LogAnomaly]:
        """æ£€æµ‹å®‰å…¨å¼‚å¸¸"""
        anomalies: list[LogAnomaly] = []
        
        for req in requests:
            url = req.get('url', '')
            body = req.get('body', '') or ''
            raw_logs = req.get('raw_logs', '') or ''
            
            content = f"{url}\n{body}\n{raw_logs}"
            
            for pattern, severity in self.SECURITY_PATTERNS:
                if re.search(pattern, content, re.IGNORECASE):
                    match = re.search(pattern, content, re.IGNORECASE)
                    matched_text = match.group(0) if match else ""
                    
                    anomaly = LogAnomaly(
                        anomaly_id=hashlib.md5(f"security:{req['request_id']}:{pattern}".encode()).hexdigest()[:16],
                        anomaly_type=AnomalyType.SECURITY_ALERT,
                        severity=severity,
                        title=f"å®‰å…¨å‘Šè­¦: {matched_text[:30]}",
                        description=f"åœ¨è¯·æ±‚ {req['method']} {req['url'][:50]} ä¸­æ£€æµ‹åˆ°æ½œåœ¨å®‰å…¨é—®é¢˜",
                        log_content=content[:500],
                        affected_endpoints=[f"{req['method']} {req['url']}"],
                        suggested_actions=[
                            "æ£€æŸ¥è¯·æ±‚æ¥æºæ˜¯å¦åˆæ³•",
                            "éªŒè¯è¾“å…¥å‚æ•°æ˜¯å¦ç»è¿‡å®‰å…¨è¿‡æ»¤",
                            "æ£€æŸ¥ç›¸å…³æ—¥å¿—ç¡®è®¤æ˜¯å¦ä¸ºæ”»å‡»è¡Œä¸º"
                        ]
                    )
                    anomalies.append(anomaly)
                    break
        
        return anomalies
    
    def _detect_error_rate_anomalies(self, requests: list[dict[str, Any]]) -> list[LogAnomaly]:
        """æ£€æµ‹é”™è¯¯ç‡å¼‚å¸¸"""
        anomalies: list[LogAnomaly] = []
        
        # æŒ‰æ¥å£åˆ†ç»„ç»Ÿè®¡
        endpoint_stats: dict[str, dict[str, int]] = defaultdict(lambda: {"total": 0, "errors": 0})
        
        for req in requests:
            key = f"{req['method']} {req['url'].split('?')[0]}"
            endpoint_stats[key]["total"] += 1
            
            status = req.get('http_status', 0)
            if status >= 400 or req.get('has_error'):
                endpoint_stats[key]["errors"] += 1
        
        # æ£€æµ‹é«˜é”™è¯¯ç‡æ¥å£
        for endpoint, stats in endpoint_stats.items():
            if stats["total"] < 5:  # æ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡
                continue
            
            error_rate = stats["errors"] / stats["total"]
            
            if error_rate > 0.5:  # é”™è¯¯ç‡è¶…è¿‡50%
                anomaly = LogAnomaly(
                    anomaly_id=hashlib.md5(f"error_rate:{endpoint}".encode()).hexdigest()[:16],
                    anomaly_type=AnomalyType.ERROR_RATE_SPIKE,
                    severity=AnomalySeverity.CRITICAL if error_rate > 0.8 else AnomalySeverity.ERROR,
                    title=f"é«˜é”™è¯¯ç‡: {error_rate:.0%}",
                    description=f"æ¥å£ {endpoint} é”™è¯¯ç‡ {error_rate:.0%} ({stats['errors']}/{stats['total']})",
                    log_content="",
                    affected_endpoints=[endpoint],
                    metadata={
                        "error_rate": error_rate,
                        "total_requests": stats["total"],
                        "error_count": stats["errors"]
                    },
                    suggested_actions=[
                        "æ£€æŸ¥æ¥å£å®ç°æ˜¯å¦æœ‰bug",
                        "æ£€æŸ¥ä¾èµ–æœåŠ¡æ˜¯å¦æ­£å¸¸",
                        "æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—å®šä½é—®é¢˜"
                    ]
                )
                anomalies.append(anomaly)
        
        return anomalies
    
    def _create_anomaly_from_line(
        self,
        line: str,
        line_number: int,
        anomaly_type: AnomalyType,
        severity: AnomalySeverity
    ) -> LogAnomaly:
        """ä»æ—¥å¿—è¡Œåˆ›å»ºå¼‚å¸¸"""
        # æå–æ—¶é—´æˆ³
        timestamp = None
        ts_match = re.search(r'\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}', line)
        if ts_match:
            try:
                timestamp = datetime.fromisoformat(ts_match.group(0).replace(' ', 'T'))
            except ValueError:
                pass
        
        return LogAnomaly(
            anomaly_id=hashlib.md5(f"{line_number}:{line[:50]}".encode()).hexdigest()[:16],
            anomaly_type=anomaly_type,
            severity=severity,
            title=self._extract_title(line, anomaly_type),
            description=line[:200],
            log_content=line,
            timestamp=timestamp,
            metadata={"line_number": line_number}
        )
    
    def _extract_title(self, line: str, anomaly_type: AnomalyType) -> str:
        """ä»æ—¥å¿—è¡Œæå–æ ‡é¢˜"""
        # å°è¯•æå–é”™è¯¯ç±»å‹
        error_match = re.search(r'(\w+(?:Error|Exception|Warning|Failed))', line)
        if error_match:
            return error_match.group(1)
        
        # æŒ‰ç±»å‹è¿”å›é»˜è®¤æ ‡é¢˜
        type_titles = {
            AnomalyType.ERROR_LOG: "é”™è¯¯æ—¥å¿—",
            AnomalyType.WARNING_LOG: "è­¦å‘Šæ—¥å¿—",
            AnomalyType.EXCEPTION: "å¼‚å¸¸",
            AnomalyType.SECURITY_ALERT: "å®‰å…¨å‘Šè­¦"
        }
        return type_titles.get(anomaly_type, "å¼‚å¸¸")
    
    def _aggregate_anomalies(self, anomalies: list[LogAnomaly]) -> list[LogAnomaly]:
        """èšåˆç›¸ä¼¼å¼‚å¸¸"""
        # æŒ‰æ ‡é¢˜åˆ†ç»„
        groups: dict[str, list[LogAnomaly]] = defaultdict(list)
        
        for anomaly in anomalies:
            # ä½¿ç”¨æ ‡é¢˜å’Œç±»å‹ä½œä¸ºåˆ†ç»„é”®
            key = f"{anomaly.anomaly_type.value}:{anomaly.title}"
            groups[key].append(anomaly)
        
        # èšåˆ
        aggregated: list[LogAnomaly] = []
        for key, group in groups.items():
            if len(group) == 1:
                aggregated.append(group[0])
            else:
                # åˆå¹¶åŒç±»å¼‚å¸¸
                first = group[0]
                all_endpoints = []
                for a in group:
                    all_endpoints.extend(a.affected_endpoints)
                
                merged = LogAnomaly(
                    anomaly_id=first.anomaly_id,
                    anomaly_type=first.anomaly_type,
                    severity=max(a.severity.value for a in group) and first.severity,  # å–æœ€é«˜ä¸¥é‡çº§åˆ«
                    title=f"{first.title} (x{len(group)})",
                    description=first.description,
                    log_content=first.log_content,
                    timestamp=first.timestamp,
                    count=len(group),
                    affected_endpoints=list(set(all_endpoints))[:10],
                    stack_trace=first.stack_trace,
                    suggested_actions=first.suggested_actions,
                    metadata={**first.metadata, "occurrence_count": len(group)}
                )
                aggregated.append(merged)
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        severity_order = {
            AnomalySeverity.CRITICAL: 0,
            AnomalySeverity.ERROR: 1,
            AnomalySeverity.WARNING: 2,
            AnomalySeverity.INFO: 3
        }
        aggregated.sort(key=lambda a: (severity_order.get(a.severity, 9), -a.count))
        
        return aggregated
    
    def _ai_analyze_anomalies(self, anomalies: list[LogAnomaly]) -> dict[str, Any]:
        """ä½¿ç”¨ AI åˆ†æå¼‚å¸¸"""
        self.logger.ai_start("AIå¼‚å¸¸åˆ†æ", f"{len(anomalies)} ä¸ªå¼‚å¸¸")
        
        # å‡†å¤‡å¼‚å¸¸æ‘˜è¦
        anomaly_summary = []
        for a in anomalies[:20]:  # é™åˆ¶æ•°é‡
            anomaly_summary.append({
                "type": a.anomaly_type.value,
                "severity": a.severity.value,
                "title": a.title,
                "description": a.description[:200],
                "count": a.count,
                "affected_endpoints": a.affected_endpoints[:5]
            })
        
        # è°ƒç”¨ AI è¯Šæ–­
        result = self.analysis_chain.diagnose_errors(
            error_logs=json.dumps(anomaly_summary, ensure_ascii=False, indent=2),
            context={
                "total_anomalies": len(anomalies),
                "critical_count": sum(1 for a in anomalies if a.severity == AnomalySeverity.CRITICAL),
                "error_count": sum(1 for a in anomalies if a.severity == AnomalySeverity.ERROR)
            }
        )
        
        self.logger.ai_end("åˆ†æå®Œæˆ")
        
        return result
    
    def _create_report(
        self,
        task_id: str,
        anomalies: list[LogAnomaly],
        ai_analysis: str | None,
        recommendations: list[str]
    ) -> AnomalyReport:
        """åˆ›å»ºå¼‚å¸¸æŠ¥å‘Š"""
        report_id = hashlib.md5(
            f"anomaly_report:{task_id}:{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]
        
        critical_count = sum(1 for a in anomalies if a.severity == AnomalySeverity.CRITICAL)
        error_count = sum(1 for a in anomalies if a.severity == AnomalySeverity.ERROR)
        warning_count = sum(1 for a in anomalies if a.severity == AnomalySeverity.WARNING)
        
        # ç”Ÿæˆæ‘˜è¦
        if critical_count > 0:
            summary = f"å‘ç° {critical_count} ä¸ªä¸¥é‡é—®é¢˜éœ€è¦ç«‹å³å¤„ç†"
        elif error_count > 0:
            summary = f"å‘ç° {error_count} ä¸ªé”™è¯¯éœ€è¦å…³æ³¨"
        elif warning_count > 0:
            summary = f"å‘ç° {warning_count} ä¸ªè­¦å‘Š"
        else:
            summary = "æœªå‘ç°æ˜æ˜¾å¼‚å¸¸"
        
        return AnomalyReport(
            report_id=report_id,
            task_id=task_id,
            title=f"æ—¥å¿—å¼‚å¸¸æŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            summary=summary,
            total_anomalies=len(anomalies),
            critical_count=critical_count,
            error_count=error_count,
            warning_count=warning_count,
            anomalies=anomalies,
            ai_analysis=ai_analysis,
            recommendations=recommendations
        )
    
    def _save_report(self, report: AnomalyReport) -> None:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ•°æ®åº“"""
        # ç”Ÿæˆ Markdown å†…å®¹
        content = self._generate_markdown_report(report)
        
        sql = """
            INSERT INTO analysis_reports 
            (task_id, report_type, title, content, format, statistics, issues, recommendations)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        statistics = {
            "total_anomalies": report.total_anomalies,
            "critical_count": report.critical_count,
            "error_count": report.error_count,
            "warning_count": report.warning_count
        }
        
        issues = [
            {
                "id": a.anomaly_id,
                "type": a.anomaly_type.value,
                "severity": a.severity.value,
                "title": a.title,
                "description": a.description[:500],
                "count": a.count
            }
            for a in report.anomalies[:50]
        ]
        
        self.db.execute(sql, (
            report.task_id,
            'analysis',
            report.title,
            content,
            'markdown',
            json.dumps(statistics, ensure_ascii=False),
            json.dumps(issues, ensure_ascii=False),
            json.dumps(report.recommendations, ensure_ascii=False)
        ))
    
    def _generate_markdown_report(self, report: AnomalyReport) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š"""
        lines = [
            f"# {report.title}",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {report.created_at.strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## æ‘˜è¦",
            "",
            report.summary,
            "",
            "## ç»Ÿè®¡",
            "",
            f"- æ€»å¼‚å¸¸æ•°: {report.total_anomalies}",
            f"- ä¸¥é‡: {report.critical_count}",
            f"- é”™è¯¯: {report.error_count}",
            f"- è­¦å‘Š: {report.warning_count}",
            "",
        ]
        
        # ä¸¥é‡é—®é¢˜
        critical_anomalies = [a for a in report.anomalies if a.severity == AnomalySeverity.CRITICAL]
        if critical_anomalies:
            lines.extend([
                "## ğŸ”´ ä¸¥é‡é—®é¢˜",
                ""
            ])
            for a in critical_anomalies[:10]:
                lines.extend([
                    f"### {a.title}",
                    "",
                    a.description,
                    "",
                    f"- ç±»å‹: {a.anomaly_type.value}",
                    f"- å‡ºç°æ¬¡æ•°: {a.count}",
                    ""
                ])
                if a.suggested_actions:
                    lines.append("**å»ºè®®æ“ä½œ**:")
                    for action in a.suggested_actions:
                        lines.append(f"- {action}")
                    lines.append("")
        
        # é”™è¯¯
        error_anomalies = [a for a in report.anomalies if a.severity == AnomalySeverity.ERROR]
        if error_anomalies:
            lines.extend([
                "## ğŸŸ  é”™è¯¯",
                ""
            ])
            for a in error_anomalies[:10]:
                lines.extend([
                    f"### {a.title}",
                    "",
                    a.description[:300],
                    "",
                    f"- å‡ºç°æ¬¡æ•°: {a.count}",
                    ""
                ])
        
        # è­¦å‘Š
        warning_anomalies = [a for a in report.anomalies if a.severity == AnomalySeverity.WARNING]
        if warning_anomalies:
            lines.extend([
                "## ğŸŸ¡ è­¦å‘Š",
                ""
            ])
            for a in warning_anomalies[:10]:
                lines.append(f"- **{a.title}**: {a.description[:100]} (x{a.count})")
            lines.append("")
        
        # AI åˆ†æ
        if report.ai_analysis:
            lines.extend([
                "## AI åˆ†æ",
                "",
                report.ai_analysis,
                ""
            ])
        
        # å»ºè®®
        if report.recommendations:
            lines.extend([
                "## æ”¹è¿›å»ºè®®",
                ""
            ])
            for i, rec in enumerate(report.recommendations, 1):
                lines.append(f"{i}. {rec}")
            lines.append("")
        
        return "\n".join(lines)
